{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgH5DX-erMFg"
      },
      "outputs": [],
      "source": [
        "# GOOGLE MAPS SCRAPER\n",
        "# Step 1: Install dependencies\n",
        "print(\"ðŸ“¦ Installing dependencies...\")\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install required Python packages\n",
        "packages = ['selenium', 'beautifulsoup4', 'requests', 'pandas']\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"{package} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        install_package(package)\n",
        "\n",
        "# Step 2: Install Chrome and ChromeDriver for Colab\n",
        "print(\"Setting up Chrome and ChromeDriver...\")\n",
        "os.system('apt-get update > /dev/null 2>&1')\n",
        "os.system('apt install chromium-chromedriver > /dev/null 2>&1')\n",
        "os.system('cp /usr/lib/chromium-browser/chromedriver /usr/bin')\n",
        "print(\"Chrome setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from urllib.parse import urlparse, urljoin, quote_plus\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import ssl\n",
        "import urllib3\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "import io\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change the name of folder to where you want to save the results in Drive\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/COLAB CLIENTS/LeadSheets\"\n",
        "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "def _running_in_colab() -> bool:\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "def _parse_start_batch(default: int = 0) -> int:\n",
        "    parser = argparse.ArgumentParser(add_help=False)\n",
        "    parser.add_argument(\"--start-batch\", type=int, default=None)\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    if args.start_batch is not None:\n",
        "        return max(0, int(args.start_batch))\n",
        "\n",
        "    env_val = os.getenv(\"START_BATCH\")\n",
        "    if env_val is not None:\n",
        "        try:\n",
        "            return max(0, int(env_val))\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    if _running_in_colab():\n",
        "        try:\n",
        "            user_in = input(f\"Last batch processed # (press Enter if want to start from begining: \").strip()\n",
        "            if user_in != \"\":\n",
        "                return max(0, int(user_in))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return max(0, int(default))\n",
        "\n",
        "START_BATCH = _parse_start_batch(0)\n",
        "\n",
        "\n",
        "# Disable SSL warnings for problematic certificates\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "\n",
        "class ZipCodeBusinessScraper:\n",
        "    def __init__(self):\n",
        "        self.all_businesses = []\n",
        "        self.processed_businesses = set()\n",
        "        self.zip_results = {}\n",
        "        self.zip_codes = []\n",
        "        self.email_cache = {}  # Cache emails to prevent re-extraction\n",
        "        self.website_cache = {}  # Cache website validation results\n",
        "        self.intermediate_files = []  # Track intermediate CSV files\n",
        "        self.csv_counter = 1  # Counter for intermediate CSV files\n",
        "        self.setup_driver()\n",
        "\n",
        "    def setup_driver(self):\n",
        "        \"\"\"Setup Chrome WebDriver\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        chrome_options.add_argument(\"--disable-gpu\")\n",
        "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "\n",
        "        # Performance optimizations\n",
        "        chrome_options.add_argument(\"--disable-images\")\n",
        "        # chrome_options.add_argument(\"--disable-javascript\")    #check this later\n",
        "        chrome_options.add_argument(\"--disable-plugins\")\n",
        "        chrome_options.add_argument(\"--disable-extensions\")\n",
        "        chrome_options.add_argument(\"--no-first-run\")\n",
        "        chrome_options.add_argument(\"--disable-default-apps\")\n",
        "\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "\n",
        "        self.driver.set_page_load_timeout(20)\n",
        "        self.driver.implicitly_wait(3)\n",
        "\n",
        "    def get_zip_codes_for_city(self, city_name, state_name=None):\n",
        "        print(f\"\\nFinding ZIP codes for {city_name}...\")\n",
        "\n",
        "        zip_codes = set()\n",
        "\n",
        "        # Method 1: ZIP code API\n",
        "        zip_codes.update(self.get_zip_codes_from_api(city_name, state_name))\n",
        "\n",
        "        # If we have enough from API, skip scraping\n",
        "        if len(zip_codes) >= 10:\n",
        "            final_zip_codes = sorted(list(zip_codes))\n",
        "            print(f\"Found {len(final_zip_codes)} ZIP codes from API\")\n",
        "            return final_zip_codes\n",
        "\n",
        "        # Method 2: Use known ZIP code ranges (backup)\n",
        "        if not zip_codes:\n",
        "            zip_codes.update(self.get_known_zip_codes(city_name))\n",
        "\n",
        "        # Method 3: Scrape only if necessary and we have few results\n",
        "        if len(zip_codes) < 5:\n",
        "            zip_codes.update(self.scrape_zip_codes_from_websites(city_name, state_name))\n",
        "\n",
        "        final_zip_codes = sorted(list(zip_codes))\n",
        "        print(f\"Found {len(final_zip_codes)} ZIP codes for {city_name}: {final_zip_codes[:10]}{'...' if len(final_zip_codes) > 10 else ''}\")\n",
        "\n",
        "        return final_zip_codes\n",
        "\n",
        "    def get_zip_codes_from_api(self, city_name, state_name):\n",
        "        zip_codes = set()\n",
        "\n",
        "        try:\n",
        "            if state_name:\n",
        "                state_abbr = self.get_state_abbreviation(state_name)\n",
        "                url = f\"http://api.zippopotam.us/us/{state_abbr}/{city_name.replace(' ', '%20')}\"\n",
        "            else:\n",
        "                url = f\"http://api.zippopotam.us/us/{city_name.replace(' ', '%20')}\"\n",
        "\n",
        "            response = requests.get(url, timeout=6)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if 'places' in data:\n",
        "                    for place in data['places']:\n",
        "                        if 'post code' in place:\n",
        "                            zip_codes.add(place['post code'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"API method failed: {e}\")\n",
        "\n",
        "        return zip_codes\n",
        "\n",
        "    def scrape_zip_codes_from_websites(self, city_name, state_name):\n",
        "        zip_codes = set()\n",
        "\n",
        "        try:\n",
        "            new_codes = self.scrape_unitedstateszipcodes_com(city_name, state_name)\n",
        "            zip_codes.update(new_codes)\n",
        "        except Exception as e:\n",
        "            print(f\"Website scraping failed: {e}\")\n",
        "\n",
        "        return zip_codes\n",
        "\n",
        "    def scrape_unitedstateszipcodes_com(self, city_name, state_name):\n",
        "        \"\"\"Scrape from unitedstateszipcodes.org\"\"\"\n",
        "        zip_codes = set()\n",
        "\n",
        "        try:\n",
        "            city_formatted = city_name.lower().replace(' ', '-')\n",
        "            state_formatted = (state_name or '').lower().replace(' ', '-')\n",
        "\n",
        "            if state_formatted:\n",
        "                url = f\"https://www.unitedstateszipcodes.org/{state_formatted}/{city_formatted}/\"\n",
        "            else:\n",
        "                likely_states = self.get_likely_states(city_name)\n",
        "                url = f\"https://www.unitedstateszipcodes.org/{likely_states[0]}/{city_formatted}/\"\n",
        "\n",
        "            response = requests.get(url, timeout=8, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            if response.status_code == 200:\n",
        "                zip_pattern = r'\\b\\d{5}(?:-\\d{4})?\\b'\n",
        "                zip_matches = re.findall(zip_pattern, response.text)\n",
        "\n",
        "                for zip_code in zip_matches:\n",
        "                    if len(zip_code) >= 5:\n",
        "                        zip_codes.add(zip_code[:5])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping unitedstateszipcodes.org: {e}\")\n",
        "\n",
        "        return zip_codes\n",
        "\n",
        "    def get_known_zip_codes(self, city_name):\n",
        "        \"\"\"Enhanced known ZIP code ranges for major cities\"\"\"\n",
        "        known_zip_ranges = {\n",
        "            'chicago': list(range(60656, 60662)) + list(range(60701, 60730)),\n",
        "            'los angeles': list(range(90001, 90099)) + list(range(91001, 91609)) +\n",
        "                          list(range(90201, 90299)) + list(range(91701, 91799)),\n",
        "            'new york': list(range(10001, 10299)) + list(range(11201, 11256)) +\n",
        "                       list(range(10301, 10314)) + list(range(11001, 11099)),\n",
        "            'houston': list(range(77001, 77099)) + list(range(77201, 77299)) +\n",
        "                      list(range(77301, 77399)),\n",
        "            'phoenix': list(range(85001, 85099)) + list(range(85201, 85299)),\n",
        "            'philadelphia': list(range(19101, 19199)) + list(range(19001, 19099)),\n",
        "            'san antonio': list(range(78201, 78299)) + list(range(78001, 78099)),\n",
        "            'san diego': list(range(92001, 92199)) + list(range(91901, 91999)),\n",
        "            'dallas': list(range(75201, 75299)) + list(range(75001, 75099)),\n",
        "            'san jose': list(range(95101, 95199)) + list(range(94001, 94099)),\n",
        "            'miami': list(range(33101, 33199)) + list(range(33001, 33099)),\n",
        "            'atlanta': list(range(30301, 30399)) + list(range(30001, 30099)),\n",
        "            'denver': list(range(80201, 80299)) + list(range(80001, 80099)),\n",
        "            'seattle': list(range(98101, 98199)) + list(range(98001, 98099))\n",
        "        }\n",
        "\n",
        "        city_lower = city_name.lower()\n",
        "        if city_lower in known_zip_ranges:\n",
        "            zip_codes = [str(code) for code in known_zip_ranges[city_lower]]\n",
        "            print(f\"Using known ZIP codes for {city_name}: {len(zip_codes)} codes\")\n",
        "            return zip_codes\n",
        "\n",
        "        return []\n",
        "\n",
        "    def get_state_abbreviation(self, state_name):\n",
        "        \"\"\"Convert state name to abbreviation\"\"\"\n",
        "        state_abbrevs = {\n",
        "            'alabama': 'AL', 'alaska': 'AK', 'arizona': 'AZ', 'arkansas': 'AR', 'california': 'CA',\n",
        "            'colorado': 'CO', 'connecticut': 'CT', 'delaware': 'DE', 'florida': 'FL', 'georgia': 'GA',\n",
        "            'hawaii': 'HI', 'idaho': 'ID', 'illinois': 'IL', 'indiana': 'IN', 'iowa': 'IA',\n",
        "            'kansas': 'KS', 'kentucky': 'KY', 'louisiana': 'LA', 'maine': 'ME', 'maryland': 'MD',\n",
        "            'massachusetts': 'MA', 'michigan': 'MI', 'minnesota': 'MN', 'mississippi': 'MS',\n",
        "            'missouri': 'MO', 'montana': 'MT', 'nebraska': 'NE', 'nevada': 'NV', 'new hampshire': 'NH',\n",
        "            'new jersey': 'NJ', 'new mexico': 'NM', 'new york': 'NY', 'north carolina': 'NC',\n",
        "            'north dakota': 'ND', 'ohio': 'OH', 'oklahoma': 'OK', 'oregon': 'OR', 'pennsylvania': 'PA',\n",
        "            'rhode island': 'RI', 'south carolina': 'SC', 'south dakota': 'SD', 'tennessee': 'TN',\n",
        "            'texas': 'TX', 'utah': 'UT', 'vermont': 'VT', 'virginia': 'VA', 'washington': 'WA',\n",
        "            'west virginia': 'WV', 'wisconsin': 'WI', 'wyoming': 'WY'\n",
        "        }\n",
        "\n",
        "        return state_abbrevs.get(state_name.lower(), state_name.upper()[:2])\n",
        "\n",
        "    def get_likely_states(self, city_name):\n",
        "        \"\"\"Get likely states for a city name\"\"\"\n",
        "        city_states = {\n",
        "            'chicago': ['illinois'],\n",
        "            'los angeles': ['california'],\n",
        "            'new york': ['new-york'],\n",
        "            'houston': ['texas'],\n",
        "            'phoenix': ['arizona'],\n",
        "            'philadelphia': ['pennsylvania'],\n",
        "            'san antonio': ['texas'],\n",
        "            'san diego': ['california'],\n",
        "            'dallas': ['texas'],\n",
        "            'san jose': ['california'],\n",
        "            'miami': ['florida'],\n",
        "            'atlanta': ['georgia'],\n",
        "            'boston': ['massachusetts'],\n",
        "            'denver': ['colorado'],\n",
        "            'seattle': ['washington']\n",
        "        }\n",
        "\n",
        "        return city_states.get(city_name.lower(), ['illinois', 'california', 'texas', 'new-york'])\n",
        "\n",
        "    def create_google_maps_urls(self, search_phrase, zip_codes):\n",
        "        \"\"\"Create Google Maps URLs for each ZIP code\"\"\"\n",
        "        print(f\"Creating Google Maps URLs for '{search_phrase}' across {len(zip_codes)} ZIP codes...\")\n",
        "\n",
        "        urls = []\n",
        "        base_url = \"https://www.google.com/maps/search/\"\n",
        "\n",
        "        for zip_code in zip_codes:\n",
        "            query = f\"{search_phrase} near {zip_code}\"\n",
        "            encoded_query = quote_plus(query)\n",
        "            full_url = f\"{base_url}{encoded_query}/\"\n",
        "\n",
        "            urls.append({\n",
        "                'url': full_url,\n",
        "                'zip_code': zip_code,\n",
        "                'query': query\n",
        "            })\n",
        "\n",
        "        print(f\"Created {len(urls)} Google Maps URLs\")\n",
        "        return urls\n",
        "\n",
        "    def scroll_to_load_businesses(self, max_scrolls=100):  # increase this later\n",
        "        try:\n",
        "            scrollable_div = WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((\n",
        "                    By.XPATH,\n",
        "                    '//div[@role=\"feed\" and @aria-label and contains(@class, \"m6QErb\")]'\n",
        "                ))\n",
        "            )\n",
        "\n",
        "            last_height = self.driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_div)\n",
        "            consecutive_same_height = 0\n",
        "\n",
        "            for i in range(max_scrolls):\n",
        "                self.driver.execute_script(\n",
        "                    \"arguments[0].scrollTop = arguments[0].scrollHeight\",\n",
        "                    scrollable_div\n",
        "                )\n",
        "\n",
        "                time.sleep(random.uniform(1.5, 2))  # Reduced from 2,3 seconds\n",
        "\n",
        "                new_height = self.driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_div)\n",
        "\n",
        "                if new_height == last_height:\n",
        "                    consecutive_same_height += 1\n",
        "                    if consecutive_same_height >= 3:\n",
        "                        break\n",
        "                else:\n",
        "                    consecutive_same_height = 0\n",
        "                    last_height = new_height\n",
        "\n",
        "            total_loaded = len(self.driver.find_elements(By.CSS_SELECTOR, \"div.Nv2PK\"))\n",
        "            print(f\"Finished scrolling. Total businesses loaded: {total_loaded}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Scrolling failed: {e}\")\n",
        "\n",
        "    def extract_business_data_from_maps(self, maps_url, zip_code, current_num, total_num):\n",
        "        print(f\"Loading Google Maps for ZIP {zip_code} ({current_num}/{total_num})...\")\n",
        "\n",
        "        try:\n",
        "            self.driver.get(maps_url)\n",
        "            time.sleep(4)  # Reduced from 8 seconds\n",
        "\n",
        "            self.scroll_to_load_businesses()\n",
        "\n",
        "            businesses = []\n",
        "\n",
        "            wait = WebDriverWait(self.driver, 8)\n",
        "\n",
        "            business_elements = []\n",
        "            for selector in ['.Nv2PK', 'div[role=\"article\"]', '.hfpxzc', 'a[data-value=\"Directions\"]']:\n",
        "                try:\n",
        "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    if elements:\n",
        "                        business_elements = elements\n",
        "                        print(f\"Found {len(elements)} business elements for ZIP {zip_code}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not business_elements:\n",
        "                print(f\"No business elements found for ZIP {zip_code}\")\n",
        "                return []\n",
        "\n",
        "            unique_businesses = []\n",
        "            processed_names_current_zip = set()\n",
        "\n",
        "            max_businesses = min(len(business_elements), 120)\n",
        "\n",
        "            for i, element in enumerate(business_elements[:max_businesses]):\n",
        "                try:\n",
        "                    print(f\"Processing business {i+1}/{max_businesses} in ZIP {zip_code}\")\n",
        "\n",
        "                    self.driver.execute_script(\n",
        "                        \"arguments[0].scrollIntoView({block: 'center', behavior: 'instant'});\",\n",
        "                        element\n",
        "                    )\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "                    try:\n",
        "                        element.click()\n",
        "                    except:\n",
        "                        self.driver.execute_script(\"arguments[0].click();\", element)\n",
        "\n",
        "                    time.sleep(3)\n",
        "\n",
        "                    business_data = self.extract_detailed_business_info()\n",
        "\n",
        "                    if business_data and business_data.get('name'):\n",
        "                        business_key_current = f\"{business_data['name']}_{business_data['address']}\"\n",
        "                        business_key_global = f\"{business_data['name']}_{business_data['address']}\"\n",
        "\n",
        "                        if (business_key_current not in processed_names_current_zip and\n",
        "                            business_key_global not in self.processed_businesses):\n",
        "\n",
        "                            processed_names_current_zip.add(business_key_current)\n",
        "                            self.processed_businesses.add(business_key_global)\n",
        "\n",
        "                            business_data['zip_code'] = zip_code\n",
        "                            business_data['source_url'] = maps_url\n",
        "\n",
        "                            unique_businesses.append(business_data)\n",
        "                            print(f\"Extracted: {business_data['name']}\")\n",
        "                        else:\n",
        "                            print(f\"Duplicate skipped: {business_data['name']}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing business {i+1} in ZIP {zip_code}: {str(e)[:50]}...\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"\\nZIP {zip_code} complete! Extracted {len(unique_businesses)} unique businesses\")\n",
        "            return unique_businesses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Main extraction failed for ZIP {zip_code}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_detailed_business_info(self):\n",
        "        business_data = {\n",
        "            'name': '',\n",
        "            'address': '',\n",
        "            'phone': '',\n",
        "            'website': '',\n",
        "            'email': ''\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            time.sleep(1.5)\n",
        "\n",
        "            # Extract business name\n",
        "            name_selectors = [\n",
        "                'h1.DUwDvf.lfPIob',\n",
        "                'h1[data-attrid=\"title\"]',\n",
        "                'h1.x3AX1-LfntMc-header-title-title',\n",
        "                '.qrShPb .fontHeadlineLarge',\n",
        "                'h1'\n",
        "            ]\n",
        "\n",
        "            for selector in name_selectors:\n",
        "                try:\n",
        "                    name_elem = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
        "                    name_text = name_elem.text.strip()\n",
        "                    if name_text and len(name_text) > 2 and 'search' not in name_text.lower():\n",
        "                        business_data['name'] = name_text\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Extract address\n",
        "            address_selectors = [\n",
        "                '[data-item-id=\"address\"] .Io6YTe',\n",
        "                '.Io6YTe.fontBodyMedium',\n",
        "                '[data-section-id=\"ad\"] .Io6YTe',\n",
        "                '.rogA2c .Io6YTe',\n",
        "                '[data-attrid=\"kc:/location/location:address\"] .Io6YTe'\n",
        "            ]\n",
        "\n",
        "            for selector in address_selectors:\n",
        "                try:\n",
        "                    addr_elem = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
        "                    addr_text = addr_elem.text.strip()\n",
        "                    if re.search(r'\\d+.*\\w+.*\\d{5}', addr_text) or any(x in addr_text.lower() for x in ['st', 'ave', 'rd', 'blvd']):\n",
        "                        business_data['address'] = addr_text\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Extract phone number\n",
        "            phone_selectors = [\n",
        "                '[data-item-id*=\"phone\"] .Io6YTe',\n",
        "                'a[href^=\"tel:\"]',\n",
        "                '.rogA2c button[data-item-id*=\"phone\"]',\n",
        "                '[data-attrid*=\"phone\"] .Io6YTe'\n",
        "            ]\n",
        "\n",
        "            for selector in phone_selectors:\n",
        "                try:\n",
        "                    phone_elems = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    for elem in phone_elems:\n",
        "                        phone_text = elem.text.strip()\n",
        "                        phone_match = re.search(r'(\\+?1?\\s*\\(?[0-9]{3}\\)?[-.\\s]*[0-9]{3}[-.\\s]*[0-9]{4})', phone_text)\n",
        "                        if phone_match:\n",
        "                            business_data['phone'] = phone_match.group(1)\n",
        "                            break\n",
        "                    if business_data['phone']:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Website extraction\n",
        "            website_selectors = [\n",
        "                '[role=\"main\"] [data-item-id=\"authority\"] a[href^=\"http\"]',\n",
        "                '[role=\"main\"] a[data-value=\"Website\"][href^=\"http\"]',\n",
        "                'a[data-value=\"Website\"]',\n",
        "                '.m6QErb [data-item-id=\"authority\"] a[href^=\"http\"]',\n",
        "                'button[aria-label*=\"Website\"]',\n",
        "                '[role=\"main\"] a[href^=\"http\"]'\n",
        "            ]\n",
        "\n",
        "            website_found = False\n",
        "            for selector in website_selectors:\n",
        "                try:\n",
        "                    website_elems = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    for elem in website_elems:\n",
        "                        href = elem.get_attribute('href')\n",
        "                        if href and self.is_valid_business_website_cached(href):\n",
        "                            if self.is_website_for_current_business(href, business_data['name']):\n",
        "                                business_data['website'] = href\n",
        "                                website_found = True\n",
        "\n",
        "                    if website_found:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Extract email with caching\n",
        "            if business_data['website'] and website_found:\n",
        "                business_data['email'] = self.extract_email_from_website_cached(\n",
        "                    business_data['website'],\n",
        "                    business_data['name']\n",
        "                )\n",
        "            else:\n",
        "                business_data['email'] = ''\n",
        "\n",
        "            return business_data if business_data['name'] else None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extract_detailed_business_info: {e}\")\n",
        "            return None\n",
        "\n",
        "    def is_valid_business_website_cached(self, url):\n",
        "        \"\"\"Cached website validation to avoid repeated checks\"\"\"\n",
        "        if url in self.website_cache:\n",
        "            return self.website_cache[url]\n",
        "\n",
        "        result = self.is_valid_business_website(url)\n",
        "        self.website_cache[url] = result\n",
        "        return result\n",
        "\n",
        "    def is_website_for_current_business(self, website_url, business_name):\n",
        "        \"\"\"Verify if the website actually belongs to the current business\"\"\"\n",
        "        try:\n",
        "            if not website_url or not business_name:\n",
        "                return False\n",
        "\n",
        "            domain = urlparse(website_url).netloc.lower()\n",
        "            business_keywords = re.findall(r'\\b\\w+\\b', business_name.lower())\n",
        "\n",
        "            common_words = {'law', 'office', 'offices', 'group', 'firm', 'pc', 'llc', 'inc', 'the', 'and', 'attorneys', 'lawyers', 'personal', 'injury', 'accident'}\n",
        "            business_keywords = [word for word in business_keywords if word not in common_words and len(word) > 2]\n",
        "\n",
        "            keyword_found = False\n",
        "            for keyword in business_keywords[:3]:\n",
        "                if keyword in domain or keyword in domain.replace('-', '').replace('_', ''):\n",
        "                    keyword_found = True\n",
        "                    break\n",
        "\n",
        "            return keyword_found\n",
        "\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def is_valid_business_website(self, url):\n",
        "        \"\"\"Check if URL is a valid business website\"\"\"\n",
        "        if not url:\n",
        "            return False\n",
        "\n",
        "        excluded_domains = [\n",
        "            'google.com', 'gstatic.com', 'googleapis.com', 'googleusercontent.com',\n",
        "            'facebook.com', 'instagram.com', 'twitter.com', 'linkedin.com',\n",
        "            'youtube.com', 'yelp.com', 'foursquare.com', 'maps.google.com',\n",
        "            't.co', 'bit.ly'\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            parsed = urlparse(url)\n",
        "            domain = parsed.netloc.lower()\n",
        "\n",
        "            for excluded in excluded_domains:\n",
        "                if excluded in domain:\n",
        "                    return False\n",
        "\n",
        "            if '.' not in domain or len(domain) < 4:\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def extract_email_from_website_cached(self, website_url, business_name=''):\n",
        "        \"\"\"Cached email extraction to avoid re-processing same websites\"\"\"\n",
        "        cache_key = f\"{website_url}_{business_name}\"\n",
        "\n",
        "        if cache_key in self.email_cache:\n",
        "            return self.email_cache[cache_key]\n",
        "\n",
        "        email = self.extract_email_from_website(website_url, business_name)\n",
        "        self.email_cache[cache_key] = email\n",
        "        return email\n",
        "\n",
        "    def extract_email_from_website(self, website_url, business_name=''):\n",
        "        try:\n",
        "            if not website_url:\n",
        "                return ''\n",
        "\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "                'Connection': 'keep-alive',\n",
        "            }\n",
        "\n",
        "            # Check fewer pages, prioritize most likely ones\n",
        "            pages_to_check = [\n",
        "                '', '/contact', '/contact-us', '/about', '/about-us', '/team', '/attorneys', '/lawyers', '/staff', '/people', '/leadership', '/our-team', '/directory', '/bios', '/management',\n",
        "                '/executives', '/office', '/locations', '/who-we-are', '/partners', '/members', '/employee-directory', '/key-people', '/bio'\n",
        "            ]\n",
        "\n",
        "            found_emails = set()\n",
        "\n",
        "            for page_path in pages_to_check:\n",
        "                try:\n",
        "                    if page_path:\n",
        "                        full_url = urljoin(website_url, page_path)\n",
        "                    else:\n",
        "                        full_url = website_url\n",
        "\n",
        "                    response = requests.get(\n",
        "                        full_url,\n",
        "                        headers=headers,\n",
        "                        timeout=10,\n",
        "                        allow_redirects=True,\n",
        "                        verify=False\n",
        "                    )\n",
        "\n",
        "                    if response.status_code != 200:\n",
        "                        continue\n",
        "\n",
        "                    content = response.text\n",
        "\n",
        "                    # email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "                    email_patterns = [\n",
        "                        # Standard email pattern\n",
        "                        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b',\n",
        "                        # Mailto links\n",
        "                        r'mailto:([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7})',\n",
        "                        # Email with spaces around @\n",
        "                        r'\\b[A-Za-z0-9._%+-]+\\s*@\\s*[A-Za-z0-9.-]+\\s*\\.\\s*[A-Z|a-z]{2,7}\\b',\n",
        "                        # Quoted emails\n",
        "                        r'[\"\\']([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7})[\"\\']',\n",
        "                        # Emails in JavaScript or data attributes\n",
        "                        r'email[\"\\']?\\s*[:=]\\s*[\"\\']?([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7})',\n",
        "                        # Contact form action emails\n",
        "                        r'action=[\"\\'][^\"\\']*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7})'\n",
        "                    ]\n",
        "\n",
        "                    for pattern in email_patterns:\n",
        "                        matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "\n",
        "                        for match in matches:\n",
        "                            email = match.strip().replace(' ', '')\n",
        "                            if email and self.is_valid_email(email, business_name):\n",
        "                                found_emails.add(email.lower())\n",
        "\n",
        "                    if found_emails:\n",
        "                        break\n",
        "\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            if found_emails:\n",
        "                email_list = list(found_emails)\n",
        "                return email_list[0]\n",
        "\n",
        "            return ''\n",
        "\n",
        "        except Exception:\n",
        "            return ''\n",
        "\n",
        "    def is_valid_email(self, email, business_name=''):\n",
        "        if not email or '@' not in email:\n",
        "            return False\n",
        "\n",
        "        email_lower = email.lower()\n",
        "\n",
        "        excluded_patterns = [\n",
        "            'noreply', 'no-reply', 'donotreply', 'do-not-reply',\n",
        "            'bounce', 'mailer-daemon', 'postmaster', 'webmaster',\n",
        "            'admin@', 'test@', 'example@', 'sample@',\n",
        "            '@example.com', '@test.com', '@localhost'\n",
        "        ]\n",
        "\n",
        "        for pattern in excluded_patterns:\n",
        "            if pattern in email_lower:\n",
        "                return False\n",
        "\n",
        "        email_regex = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}$'\n",
        "\n",
        "        if not re.match(email_regex, email):\n",
        "            return False\n",
        "\n",
        "        if len(email) > 100:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def save_intermediate_csv(self, businesses, search_phrase, city_name):\n",
        "        if not businesses:\n",
        "            return None\n",
        "\n",
        "        # Generate filename for intermediate CSV\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        safe_phrase = re.sub(r'[^\\w\\s-]', '', search_phrase).strip().replace(' ', '_')\n",
        "        safe_city = re.sub(r'[^\\w\\s-]', '', city_name).strip().replace(' ', '_')\n",
        "        filename = os.path.join(self.save_dir, f\"{safe_phrase}_{safe_city}_part_{self.csv_counter}.csv\")\n",
        "\n",
        "        # Save to CSV\n",
        "        df = pd.DataFrame(businesses)\n",
        "        column_order = ['name', 'address', 'phone', 'website', 'email']\n",
        "        df = df.reindex(columns=column_order)\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"INTERMEDIATE CSV #{self.csv_counter} SAVED\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"File: {filename}\")\n",
        "        print(f\"Businesses in this file: {len(businesses)}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Track the file and increment counter\n",
        "        self.intermediate_files.append(filename)\n",
        "        self.csv_counter += 1\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def scrape_zip_code_area(self, search_phrase, city_name, state_name=None, max_businesses_per_zip=120,\n",
        "                             output_file=None, batch_size=10):\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        print(f\"Last batch (0): {START_BATCH}\")\n",
        "\n",
        "        folder_name = f\"{search_phrase} in {city_name}\".strip()\n",
        "        self.save_dir = os.path.join(SAVE_ROOT, folder_name)\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "        # Step 1: Get all ZIP codes for the city\n",
        "        self.zip_codes = self.get_zip_codes_for_city(city_name, state_name)\n",
        "\n",
        "        if not self.zip_codes:\n",
        "            print(f\"ERROR: No ZIP codes found for {city_name}\")\n",
        "            return []\n",
        "\n",
        "        # Step 2: Create Google Maps URLs for each ZIP code\n",
        "        url_data = self.create_google_maps_urls(search_phrase, self.zip_codes)\n",
        "\n",
        "        # Step 3: Process ZIP codes with intermediate CSV creation every <batch_size> ZIP codes\n",
        "        total_zips = len(url_data)\n",
        "        total_batches = (total_zips + batch_size - 1) // batch_size\n",
        "\n",
        "        # ----------------------- start at specific batch -----------------------\n",
        "        start_batch_index = min(max(START_BATCH, 0), max(total_batches - 1, 0))\n",
        "        start_zip_index = start_batch_index * batch_size\n",
        "        processed_count = start_zip_index  # continue global ZIP counter correctly\n",
        "        # keep CSV part numbering aligned with every-<batch_size> ZIP cadence\n",
        "        self.csv_counter = (processed_count // batch_size) + 1\n",
        "        print(f\"Resuming at batch {start_batch_index + 1}/{total_batches} (ZIP index {start_zip_index + 1})\")\n",
        "        # ---------------------------------------------------------------------------\n",
        "\n",
        "        zip_group_businesses = []  # Businesses for current group of <batch_size> ZIP codes\n",
        "\n",
        "        try:\n",
        "            for i in range(start_zip_index, total_zips, batch_size):\n",
        "                batch = url_data[i:i + batch_size]\n",
        "                batch_num = (i // batch_size) + 1\n",
        "                total_batches = (total_zips + batch_size - 1) // batch_size\n",
        "\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(f\"PROCESSING BATCH {batch_num}/{total_batches}\")\n",
        "                print(f\"ZIP codes {i+1}-{min(i+batch_size, total_zips)} of {total_zips}\")\n",
        "                print(f\"{'='*60}\")\n",
        "\n",
        "                # Process each ZIP code in the batch\n",
        "                for j, url_info in enumerate(batch):\n",
        "                    zip_code = url_info['zip_code']\n",
        "                    maps_url = url_info['url']\n",
        "                    query = url_info['query']\n",
        "\n",
        "                    processed_count += 1\n",
        "\n",
        "                    print(f\"\\n[{processed_count}/{total_zips}] Processing ZIP {zip_code}\")\n",
        "                    print(f\"Query: {query}\")\n",
        "                    print(f\"URL: {maps_url[:80]}...\")\n",
        "\n",
        "                    try:\n",
        "                        # Extract businesses for this ZIP code\n",
        "                        businesses = self.extract_business_data_from_maps(\n",
        "                            maps_url, zip_code, processed_count, total_zips\n",
        "                        )\n",
        "\n",
        "                        # Store results\n",
        "                        self.zip_results[zip_code] = {\n",
        "                            'zip_code': zip_code,\n",
        "                            'url': maps_url,\n",
        "                            'query': query,\n",
        "                            'businesses_found': len(businesses),\n",
        "                            'businesses': businesses\n",
        "                        }\n",
        "\n",
        "                        # Add to both combined results and current group\n",
        "                        self.all_businesses.extend(businesses)\n",
        "                        zip_group_businesses.extend(businesses)\n",
        "\n",
        "                        print(f\"ZIP {zip_code} complete: {len(businesses)} businesses\")\n",
        "                        print(f\"Total businesses so far: {len(self.all_businesses)}\")\n",
        "\n",
        "                        # Check if we've processed <batch_size> ZIP codes and create intermediate CSV\n",
        "                        if processed_count % batch_size == 0 and zip_group_businesses:\n",
        "                            print(f\"\\n{'='*50}\")\n",
        "                            print(f\"CREATING INTERMEDIATE CSV AFTER {processed_count} ZIP CODES\")\n",
        "                            print(f\"{'='*50}\")\n",
        "\n",
        "                            # Clean and deduplicate current group\n",
        "                            cleaned_group = self.clean_and_deduplicate(zip_group_businesses.copy())\n",
        "\n",
        "                            # Save intermediate CSV\n",
        "                            intermediate_file = self.save_intermediate_csv(\n",
        "                                cleaned_group, search_phrase, city_name\n",
        "                            )\n",
        "\n",
        "                            print(f\"Intermediate file saved: {intermediate_file}\")\n",
        "                            print(f\"Businesses in this group: {len(cleaned_group)}\")\n",
        "\n",
        "                            # Reset group for next <batch_size> ZIP codes\n",
        "                            zip_group_businesses = []\n",
        "                            print(f\"{'='*50}\")\n",
        "\n",
        "                        if processed_count < total_zips:\n",
        "                            delay = random.uniform(2, 4)\n",
        "                            print(f\"Waiting {delay:.1f}s before next ZIP code...\")\n",
        "                            time.sleep(delay)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"ERROR processing ZIP {zip_code}: {str(e)[:100]}...\")\n",
        "                        self.zip_results[zip_code] = {\n",
        "                            'zip_code': zip_code,\n",
        "                            'url': maps_url,\n",
        "                            'query': query,\n",
        "                            'businesses_found': 0,\n",
        "                            'businesses': [],\n",
        "                            'error': str(e)\n",
        "                        }\n",
        "                        continue\n",
        "\n",
        "                # Shorter break between batches\n",
        "                if batch_num < total_batches:\n",
        "                    print(f\"\\nBatch {batch_num} complete. Taking 8-second break...\")\n",
        "                    time.sleep(8)\n",
        "\n",
        "            # Save any remaining businesses in the last group (if not a multiple of batch_size)\n",
        "            if zip_group_businesses:\n",
        "                print(f\"\\n{'='*50}\")\n",
        "                print(f\"CREATING FINAL INTERMEDIATE CSV FOR REMAINING ZIP CODES\")\n",
        "                print(f\"{'='*50}\")\n",
        "\n",
        "                cleaned_group = self.clean_and_deduplicate(zip_group_businesses.copy())\n",
        "                intermediate_file = self.save_intermediate_csv(\n",
        "                    cleaned_group, search_phrase, city_name\n",
        "                )\n",
        "                print(f\"Final intermediate file saved: {intermediate_file}\")\n",
        "                print(f\"{'='*50}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nScraping interrupted by user. Saving partial results...\")\n",
        "            # Save any remaining businesses if interrupted\n",
        "            if zip_group_businesses:\n",
        "                cleaned_group = self.clean_and_deduplicate(zip_group_businesses.copy())\n",
        "                self.save_intermediate_csv(cleaned_group, search_phrase, city_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nUnexpected error: {e}\")\n",
        "\n",
        "        finally:\n",
        "            # Process and save final combined results\n",
        "            if self.all_businesses:\n",
        "                print(f\"\\n{'='*80}\")\n",
        "                print(\"PROCESSING FINAL COMBINED RESULTS\")\n",
        "\n",
        "                # Clean and deduplicate all businesses\n",
        "                cleaned_businesses = self.clean_and_deduplicate(self.all_businesses)\n",
        "\n",
        "                # Generate output filename if not provided\n",
        "                if not output_file:\n",
        "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                    safe_phrase = re.sub(r'[^\\w\\s-]', '', search_phrase).strip().replace(' ', '_')\n",
        "                    safe_city = re.sub(r'[^\\w\\s-]', '', city_name).strip().replace(' ', '_')\n",
        "                    output_file = f\"{safe_phrase}_{safe_city}_FINAL_{timestamp}.csv\"\n",
        "\n",
        "                # Save final combined CSV\n",
        "                df = self.save_results_to_csv(cleaned_businesses, output_file)\n",
        "\n",
        "                # Print comprehensive summary including intermediate files\n",
        "                self.print_comprehensive_summary_with_intermediates(\n",
        "                    cleaned_businesses, start_time, search_phrase, city_name\n",
        "                )\n",
        "\n",
        "                return cleaned_businesses\n",
        "\n",
        "            else:\n",
        "                print(\"No businesses found across all ZIP codes\")\n",
        "                return []\n",
        "\n",
        "            # Clean up\n",
        "            if hasattr(self, 'driver'):\n",
        "                self.driver.quit()\n",
        "\n",
        "    def clean_and_deduplicate(self, businesses):\n",
        "        \"\"\"Clean data and remove duplicates across all ZIP codes\"\"\"\n",
        "        print(\"Cleaning and deduplicating businesses...\")\n",
        "\n",
        "        cleaned_businesses = []\n",
        "        seen_combinations = set()\n",
        "\n",
        "        for business in businesses:\n",
        "            name = business.get('name', '').strip()\n",
        "            address = business.get('address', '').strip()\n",
        "\n",
        "            if len(name) < 3:\n",
        "                continue\n",
        "\n",
        "            unique_key = f\"{name.lower()}_{address.lower()}\"\n",
        "\n",
        "            if unique_key in seen_combinations:\n",
        "                continue\n",
        "\n",
        "            seen_combinations.add(unique_key)\n",
        "\n",
        "            cleaned_business = {\n",
        "                'name': name,\n",
        "                'address': address,\n",
        "                'phone': business.get('phone', '').strip(),\n",
        "                'website': business.get('website', '').strip(),\n",
        "                'email': business.get('email', '').strip(),\n",
        "                'zip_code': business.get('zip_code', '').strip(),\n",
        "                'source_url': business.get('source_url', '').strip()\n",
        "            }\n",
        "\n",
        "            cleaned_businesses.append(cleaned_business)\n",
        "\n",
        "        print(f\"Cleaned: {len(businesses)} â†’ {len(cleaned_businesses)} (removed {len(businesses) - len(cleaned_businesses)} duplicates)\")\n",
        "        return cleaned_businesses\n",
        "\n",
        "    def save_results_to_csv(self, businesses, filename):\n",
        "        \"\"\"Save results to CSV with proper formatting\"\"\"\n",
        "        if not businesses:\n",
        "            print(\"No business data to save\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(businesses)\n",
        "\n",
        "        column_order = ['name', 'address', 'phone', 'website', 'email']\n",
        "        df = df.reindex(columns=column_order)\n",
        "\n",
        "        filename = os.path.join(self.save_dir, os.path.basename(filename))\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Saved {len(businesses)} businesses to {filename}\")\n",
        "\n",
        "        # Show sample data\n",
        "        print(\"\\nSample data:\")\n",
        "        sample_df = df.head(3)[['name', 'address', 'phone', 'website', 'email']]\n",
        "        for col in sample_df.columns:\n",
        "            sample_df[col] = sample_df[col].astype(str).str[:40] + '...'\n",
        "        print(sample_df.to_string(index=False))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def print_comprehensive_summary_with_intermediates(self, businesses, start_time, search_phrase, city_name):\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"FINAL COMPREHENSIVE SCRAPING SUMMARY\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        total_businesses = len(businesses)\n",
        "        with_websites = len([b for b in businesses if b['website']])\n",
        "        with_emails = len([b for b in businesses if b['email']])\n",
        "        with_phones = len([b for b in businesses if b['phone']])\n",
        "        with_addresses = len([b for b in businesses if b['address']])\n",
        "\n",
        "        print(f\"Search phrase: {search_phrase}\")\n",
        "        print(f\"City: {city_name}\")\n",
        "        print(f\"Total processing time: {duration}\")\n",
        "        print(f\"ZIP codes processed: {len(self.zip_results)}\")\n",
        "        print(f\"Total unique businesses: {total_businesses}\")\n",
        "        print(f\"Businesses with websites: {with_websites} ({with_websites/max(1,total_businesses)*100:.1f}%)\")\n",
        "        print(f\"Businesses with emails: {with_emails} ({with_emails/max(1,total_businesses)*100:.1f}%)\")\n",
        "        print(f\"Businesses with phones: {with_phones} ({with_phones/max(1,total_businesses)*100:.1f}%)\")\n",
        "        print(f\"Businesses with addresses: {with_addresses} ({with_addresses/max(1,total_businesses)*100:.1f}%)\")\n",
        "\n",
        "        # Intermediate files summary\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"INTERMEDIATE CSV FILES CREATED\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Total intermediate files: {len(self.intermediate_files)}\")\n",
        "        for i, filename in enumerate(self.intermediate_files, 1):\n",
        "            print(f\"  {i}. {filename}\")\n",
        "            if os.path.exists(filename):\n",
        "                try:\n",
        "                    df_temp = pd.read_csv(filename)\n",
        "                    print(f\"     â†’ {len(df_temp)} businesses\")\n",
        "                except:\n",
        "                    print(f\"     â†’ File exists\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Performance metrics\n",
        "        if len(self.zip_results) > 0:\n",
        "            avg_time_per_zip = duration.total_seconds() / len(self.zip_results)\n",
        "            avg_businesses_per_zip = total_businesses / len(self.zip_results)\n",
        "            print(f\"Average time per ZIP code: {avg_time_per_zip:.1f} seconds\")\n",
        "            print(f\"Average businesses per ZIP: {avg_businesses_per_zip:.1f}\")\n",
        "\n",
        "        if with_websites > 0:\n",
        "            email_success_rate = with_emails / with_websites * 100\n",
        "            print(f\"Email extraction success rate: {email_success_rate:.1f}%\")\n",
        "\n",
        "        # Cache efficiency metrics\n",
        "        print(f\"Email cache size: {len(self.email_cache)} entries\")\n",
        "        print(f\"Website validation cache size: {len(self.website_cache)} entries\")\n",
        "\n",
        "        # ZIP code breakdown\n",
        "        print(f\"\\nTOP 10 ZIP CODES BY BUSINESS COUNT:\")\n",
        "        zip_counts = {}\n",
        "        for business in businesses:\n",
        "            zip_code = business.get('zip_code', 'Unknown')\n",
        "            zip_counts[zip_code] = zip_counts.get(zip_code, 0) + 1\n",
        "\n",
        "        sorted_zips = sorted(zip_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        for zip_code, count in sorted_zips[:10]:\n",
        "            print(f\"  {zip_code}: {count} businesses\")\n",
        "\n",
        "        print(f\"\\nQUALITY METRICS:\")\n",
        "        if total_businesses > 0:\n",
        "            complete_profiles = len([b for b in businesses if all([b['name'], b['address'], b['phone'], b['website'], b['email']])])\n",
        "            print(f\"Complete profiles (all fields): {complete_profiles} ({complete_profiles/total_businesses*100:.1f}%)\")\n",
        "\n",
        "            useful_profiles = len([b for b in businesses if b['name'] and (b['phone'] or b['website'] or b['email'])])\n",
        "            print(f\"Useful profiles (name + contact): {useful_profiles} ({useful_profiles/total_businesses*100:.1f}%)\")\n",
        "\n",
        "\n",
        "        print(f\"Intermediate CSV files: {len(self.intermediate_files)}\")\n",
        "        for i, filename in enumerate(self.intermediate_files, 1):\n",
        "            print(f\"  Part {i}: {filename}\")\n",
        "        print(f\"Final combined CSV: Contains all {total_businesses} unique businesses\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "def scrape_by_zip_codes(search_phrase, city_name, state_name=None, max_businesses_per_zip=120,\n",
        "                                  output_file=None, batch_size=5):\n",
        "\n",
        "    scraper = ZipCodeBusinessScraper()\n",
        "    return scraper.scrape_zip_code_area(\n",
        "        search_phrase=search_phrase,\n",
        "        city_name=city_name,\n",
        "        state_name=state_name,\n",
        "        max_businesses_per_zip=max_businesses_per_zip,\n",
        "        output_file=output_file,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    search_phrase = input(\"Enter search phrase: \").strip().lower()\n",
        "    city_name = input(\"Enter city name: \").strip().lower()\n",
        "    state_name = input(\"Enter state name: \").strip().lower()\n",
        "\n",
        "    batch_size = 5 #default is 5\n",
        "    max_businesses_per_zip = 120\n",
        "\n",
        "    output_file = f\"{search_phrase}_{city_name}.csv\".replace(\" \", \"_\")\n",
        "\n",
        "    total_businesses = scrape_by_zip_codes(\n",
        "        search_phrase=search_phrase,\n",
        "        city_name=city_name,\n",
        "        state_name=state_name,\n",
        "        max_businesses_per_zip=max_businesses_per_zip,\n",
        "        batch_size=batch_size,\n",
        "        output_file=output_file\n",
        "    )\n",
        "\n",
        "    print(f\"\\nData saved to: {output_file}\")\n",
        "\n",
        "    print(f\"\\nScraping Complete!\")\n",
        "    print(f\"Found {len(total_businesses)} businesses!\")\n"
      ],
      "metadata": {
        "id": "unES9YD-rP_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5-2abautkfY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}